[{"title":"Browser Test and Continuous Integration","description":"<p>When developing a web app, it is important to verify that the app works with various web browers. A very comprehensive way to do this is to use a Selenium Grid with a wide selection of web browsers  connected to the grid. A complementary approach is to ensure that the continuous integration includes running the tests with the latest version of the most popular desktop browsers.</p>\n\n<p>A setup to run Chrome, Firefox, Edge, and Safari to run your app tests is easier than you imagine. Even better, a few populer hosted continuous integration system offers multi-platform support. It is necessary since there is no single operating system that cover all the above browsers. Edge will always need Windows and Safari can run only on macOS.</p>\n\n<p>In this blog post, and its accompanying git repository at <a href=\"https://github.com/ariya/evergreen-browser-tests\">github.com/ariya/evergreen-browser-tests</a>, we discuss the use of three hosted CI systems: <a href=\"https://travis-ci.org\">Travis ","link":"https://ariya.io/2018/12/browser-test-and-continuous-integration","owner":"Ariejan de Vroom"},{"title":"Twin training: trick for better model comparisons","description":"<p>Abstract: <em>Frequently comparing deep learning models?<br />\nA simple way to improve comparison is discussed here, \nthis trick becomes specially handy when comparing segmentation models.</em></p>\n\n<p>Reliable comparison of models is a question important for DL “theorists” (to evaluate new approaches) \nas well as for practitioners/engineers (to select an approach for a particular task in hand).\nComparison is time-consuming process, frequently with noisy results.</p>\n\n<p>Usual setting incorporates fixed dataset split into train/val/test and fixed metric of choice. \nNext, independent runs are conducted for all models under comparison and achieved quality is registered.</p>\n\n<p>As a result,</p>\n\n<ul>\n  <li>There is a significant noise in comparison (it is rare to rerun each model several times, specially in applications),</li>\n  <li>Validation can be done only using whole dataset</li>\n  <li>need to remember which version of code was used to generate a particular number, as you can \nac","link":"https://arogozhnikov.github.io/2019/01/01/trick-for-model-comparison.html","owner":"Alan Storm"}]