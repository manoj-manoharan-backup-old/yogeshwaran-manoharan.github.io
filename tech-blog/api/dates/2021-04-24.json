[{"title":"‘mvwin_wchnstr’ was not declared in this scope when compiling lnav","description":null,"link":"http://redino.net/blog/2021/04/mvwin_wchnstr-was-not-declared-in-this-scope-when-compiling-lnav/?utm_source=rss&utm_medium=rss&utm_campaign=mvwin_wchnstr-was-not-declared-in-this-scope-when-compiling-lnav","owner":"Redino"},{"title":"Isn't the 'Reverse Engineering for Beginners' book outdated already?","description":"Isn't the 'Reverse Engineering for Beginners' book outdated already?","link":"https://yurichev.com/news/20210424_outdated_RE4B/","owner":"Dennis Yurichev"},{"title":"Public Algo Hour, May 25th, 2021 - Step sizes for SGD: Adaptivity and Convergence | Xiaoyu Li","description":"<h3 id=\"title-step-sizes-for-sgd-adaptivity-and-convergence\">Title: Step sizes for SGD: Adaptivity and Convergence</h3>\n\n<p><img src=\"/assets/posts/2021-05-04-xiaoyu-li-algo-hour/SGD-overshooting.png\" alt=\"Overshooting\" /></p>\n\n<h3 id=\"talk-abstract\">Talk Abstract:</h3>\n\n<p>Stochastic gradient descent (SGD) has been a popular tool in training large-scale machine learning models. Yet, its performance is highly variable, depending heavily on the choice of the step sizes. This has motivated a variety of strategies for tuning the step size and research on adaptive step sizes. However, most of them lack a theoretical guarantee. In this talk, I will present a generalized AdaGrad method with adaptive step size and two heuristic step schedules for SGD: the exponential step size and cosine step size. For the first time, we provide theoretical support for them, deriving convergence guarantees and showing that these step sizes allow to automatically adapt to the level of noise of the stochastic g","link":"https://multithreaded.stitchfix.com/blog/2021/04/24/xiaoyu-li-algo-hour-announcement/","owner":"Stitch Fix"}]