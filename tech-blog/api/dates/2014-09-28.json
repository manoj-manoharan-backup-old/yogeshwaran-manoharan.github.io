[{"title":"Getting the current filename with Spark and HDFS","description":"<p>It’s occasionally useful when writing map/reduce jobs to get a hold of the current filename that’s being processed.\nThere’s a few ways to do this, depending on the version of Spark that you’re using.</p>\n\n<p>Spark 1.1.0 introduced a new method on <a href=\"https://spark.apache.org/docs/1.1.0/api/scala/index.html#org.apache.spark.rdd.HadoopRDD\">HadoopRDD</a>\nthat makes this super easy:</p>\n\n<div class=\"language-scala highlighter-rouge\"><pre class=\"highlight\"><code><span class=\"k\">import</span> <span class=\"nn\">org.apache.hadoop.io.LongWritable</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.hadoop.io.Text</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.hadoop.mapred.</span><span class=\"o\">{</span><span class=\"nc\">FileSplit</span><span class=\"o\">,</span> <span class=\"nc\">TextInputFormat</span><span class=\"o\">}</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.rdd.HadoopRDD</span>\n\n<span class=\"c1\">// Create the text file\n</span><sp","link":"http://themodernlife.net/scala/spark/hadoop/hdfs/2014/09/28/spark-input-filename/","owner":"Ian Hummel"}]