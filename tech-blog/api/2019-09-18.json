[{"title":"A Common Gotcha with Asynchronous GPU Computing","description":"<p>\nFor the most part, computing libraries such as  <a href=\"https://neanderthal.uncomplicate.org\">Neanderthal</a> (<iframe class=\"github-btn\" src=\"https://ghbtns.com/github-btn.html?user=uncomplicate&amp;repo=neanderthal&amp;type=watch&amp;count=true\" width=\"100\" height=\"20\" title=\"Star on GitHub\" frameBorder=\"0\"></iframe>)\nabstract the complexity of GPU computing. When you have high-level linear algebra operations,\nsuch as <code>mm!</code> (matrix multiply), <code>nrm2</code> (vector/matrix norm), or <code>sum</code>, you do not have to\nworry about compiling kernels, loading modules, sending operations to the right stream in\nthe right context, handling stream execution errors, and a bunch of other details.\nThe code we write and test on the CPU can be executed on the GPU!\n</p>\n\n<p>\nHowever, there is one <i>important</i> thing we have to keep in mind: GPU computing\nroutines are asynchronous by default! Most of the time we see the benefits of\nsuch behavior, but in some common situations","link":"http://dragan.rocks/articles/19/Common-Gotcha-Asynchronous-GPU-CUDA-Computing-Clojure","owner":"Dot Dev (.dev)"}]